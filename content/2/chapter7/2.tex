Testing is the staple diet for any software engineer who takes pride in quality software. The number of frameworks for writing unit tests in the various languages is huge and, especially for C++, CMake includes modules to work with most of the more popular ones.

At very abstract levels, all unit testing frameworks do the following:

\begin{itemize}
\item 
Allow the formulating and grouping of test cases.

\item 
Contain some form of assertion to check for various test conditions.

\item 
Discover and run the test cases, either altogether or a selection of them.

\item 
Produce the test result in a variety of formats, such as plain text, JSON, XML, and possibly more.
\end{itemize}

With the CTest utility, CMake includes a built-in way to execute almost any test. Any CMake project that has set enable\_testing() and added at least one test with add\_test() has testing support enabled. Any call to enable\_testing() will enable test discovery in the current directory and any directory below, so it is often a good idea to set it in the top-level CMakeLists.txt, before any calls to add\_subdirectory. The CTest module of CMake automatically sets enable\_testing if used with include(CTest), unless the BUILD\_TESTING option was set to OFF.

It is good practice to disable building and running the tests depending on the BUILD\_TESTING option. A common pattern here is to put all parts of a project that concern testing into its own subfolder and only include the subfolder if BUILD\_TESTING is set to ON.

The CTest module should generally be included only in the top-level CMakeLists.txt of a project. Since CMake version 3.21, the PROJECT\_IS\_TOP\_LEVEL variable can be used to test if the current CMakeLists.txt is the top level. This variable will be true for the top-level directory of a project and top-level directories of projects added with ExternalProject. For directories added with add\_subdirectory or FetchContent, the value is false. As such, CTest should be included like this:

\begin{lstlisting}[style=styleCMake]
project(CMakeBestPractice)
...
if(PROJECT_IS_TOP_LEVEL)
include(CTest)
endif()
\end{lstlisting}

nit tests are, in essence, small programs that run a list of assertions inside, and if any of the assertions fail, they return a non-zero return value. There are many frameworks and libraries that help with organizing tests and writing assertions, but from the outside, checking assertions and returning a corresponding value is the core functionality.

Tests can be added to any CMakeLists.txt with the add\_test function:

\begin{lstlisting}[style=styleCMake]
add_test(NAME <name> COMMAND <command> [<arg>...]
	[CONFIGURATIONS <config>...]
	[WORKING_DIRECTORY <dir>]
	[COMMAND_EXPAND_LISTS])
\end{lstlisting}

COMMAND can be the name of an executable target defined in the project or a full path to an arbitrary executable. Any arguments needed for the test are also included. Using target names is the preferred way, as CMake will then substitute the path to the executable automatically. The CONFIGURATION option is used to tell CMake for which build configurations the test is valid. For most test cases, this is irrelevant, but for microbenchmarking, for instance, this can be quite useful. WORKING\_DIRECTORY should be an absolute path. By default, tests are executed in CMAKE\_CURRENT\_BINARY\_DIR. COMMAND\_EXPAND\_LISTS ensures that any lists passed as part of the COMMAND option are expanded.

A simple project including a test might look like this:

\begin{lstlisting}[style=styleCMake]
cmake_minimum_required(VERSION 3.21)
project("simple_test" VERSION 1.0)
enable_testing()
add_executable(simple_test)
target_sources(simple_test PRIVATE src/main.cpp)
add_test(NAME example_test COMMAND simple_test)
\end{lstlisting}

In the example, an executable target called simple\_test is used as a test called example\_test.

CTest will consume the information about the tests and execute them. The tests are executed by running the ctest command standalone or as a special target as part of the build step of CMake. Either of the two following commands will execute the tests:

\begin{tcblisting}{commandshell={}}
ctest --test-dir <build_dir>
cmake --build <build_dir> --target test
\end{tcblisting}

Invoking CTest as a target of the build has the advantage that CMake will check first whether all the needed targets are built and on the newest version, but CTest can do that as well, like this:

\begin{tcblisting}{commandshell={}}
ctest --build-and-test <source_dir> <build_dir>
\end{tcblisting}

The output of ctest might look something like this:

\begin{tcblisting}{commandshell={}}
Test project /workspaces/CMake-Best-Practices/build
	Start 1: example_test
1/3 Test #1: example_test .....................***Failed
0.00 sec
	Start 2: pass_fail_test
2/3 Test #2: pass_fail_test ................... Passed
0.00 sec
	Start 3: timeout_test
3/3 Test #3: timeout_test ..................... Passed
0.50 sec

67% tests passed, 1 tests failed out of 3

Total Test time (real) = 0.51 sec

The following tests FAILED:
         1 - example_test (Failed)
Errors while running CTest
Output from these tests are in:
     /workspaces/CMake-BestPractices/build/Testing/Temporary/LastTest.log
Use "--rerun-failed --output-on-failure" to re-run the failed
     cases verbosely.
\end{tcblisting}

Generally, the test suppresses all output to stdout. By passing the -V or -{}-verbose command-line argument, the output is always printed. However, usually, you're only interested in the output of the failed tests. So, the -{}-output-on-failure argument is often the better alternative. This way, only failed tests produce output. For very verbose tests, the output can be limited in size with the -{}-test-output-size-passed <size> and -{}-test-output-size-failed <size> options, where the size is the number of bytes.

Having one or more calls to add\_test in the build tree will cause CMake to write out an input file for CTest in CMAKE\_CURRENT\_BINARY\_DIR. The input files for CTest are not necessarily located at the top level of the project, but where they are defined. To list all tests but not execute them, the -N option for CTest is used.

A very useful feature of CTest is that it caches the states of the tests between runs. This allows you to only run tests that failed in the last run. For this, running ctest -{}-rerun-failed will just run the tests that failed in the last run.

Sometimes, you do not want to execute the full test set, for instance, if a single failing test is to be fixed. The -E and -R command-line options take regular expressions (regexes) that are matched against test names. The -E option excludes tests matching the pattern, and the -R option selects tests to be included. The options can be combined. The following command would run all tests that begin with FeatureX but exclude the test called FeatureX\_Test\_1:

\begin{tcblisting}{commandshell={}}
ctest -R ^FeatureX -E FeatureX_Test_1
\end{tcblisting}

Another way to selectively execute tests is to label them using the LABELS properties for tests and then select the labels to run with the -L option of CTest. A test can have multiple labels assigned separated by a semicolon, as shown in the following example:

\begin{lstlisting}[style=styleCMake]
add_test(NAME labeled_test_1 COMMAND someTest)
set_tests_properties(labeled_test PROPERTIES LABELS "example")

add_test(NAME labeled_test_2 COMMAND anotherTest)
set_tests_properties(labeled_test_2 PROPERTIES LABELS "will_
	fail" )

add_test(NAME labeled_test_3 COMMAND YetAnotherText)
set_tests_properties(labeled_test_3 PROPERTIES LABELS
	"example;will_fail")
\end{lstlisting}

The -L command line option takes a regex to filter for the labels:

\begin{tcblisting}{commandshell={}}
ctest -L example
\end{tcblisting}

This will only execute labeled\_test\_1 and labeled\_test\_3, as they both have the example label assigned, but not labeled\_test\_2 or any other tests that have no label assigned.

By formulating the regex accordingly, multiple labels can be combined:

\begin{tcblisting}{commandshell={}}
ctest -L "example|will_fail"
\end{tcblisting}

This would execute all the tests from the example, but no other tests that have no label assigned.

Using labels would be particularly useful to mark tests that are designed to fail or similar, or to mark tests that are only relevant in certain execution contexts.

The last alternative to the regex or label-based test selection is to use the -I option, which takes the assigned test numbers. The argument for the -I option is somewhat complicated:

\begin{tcblisting}{commandshell={}}
ctest -I [Start,End,Stride,test#,test#,...|Test file]
\end{tcblisting}

With Start, End, and Stride, a range for the tests to be executed can be specified. The three numbers are for the range combined with explicit test numbers, test\#. Alternatively, a file containing the argument can be passed.

The following call would execute all odd tests from 1 to 10:

\begin{tcblisting}{commandshell={}}
ctest -I 1,10,2
\end{tcblisting}

So, tests 1, 3, 5, 7, and 9 would be executed. The following command would execute only the tests and 8:

\begin{tcblisting}{commandshell={}}
ctest -I ,0,,6,7
\end{tcblisting}

Note that in this call, End is set to 0 so no test range is executed. To combine the range and explicit test numbers, the following command will execute all odd tests from 1 to 10, and additionally test 6 and 8:

\begin{tcblisting}{commandshell={}}
ctest -I 1,10,2,6,8
\end{tcblisting}

The cumbersome handling of the -I option and the fact that adding new tests might reassign the numbers are two reasons why it is rarely used in practice. Usually, filtering either by labels or test names is preferred.

Another common pitfall when writing tests is that they are not independent enough. So, test 2 might accidentally depend on a previous execution of test 1. To harden against this accidental dependency, CTest has the ability to randomize test execution order with the -{}-schedule-random command-line argument. This will ensure that tests are executed in an arbitrary order.

\subsubsubsection{7.2.1\hspace{0.2cm}Automatically discovering tests}

Defining tests with add\_test is one way to expose them to CTest. One drawback is that this will register the whole executable as a single test. In most cases, however, a single executable will contain many unit tests and not just one, so when one of the tests inside the executable fails, it might be hard to figure out which test exactly failed.

Consider a C++ file containing the following test code, and let's assume that the Fibonacci function contains a bug, so Fibonacci(0) will not return 1 as it should, but returns something else:

\begin{lstlisting}[style=styleCXX]
TEST_CASE("Fibonacci(0) returns 1"){ REQUIRE(Fibonacci(0) ==
	1);}
TEST_CASE("Fibonacci(1) returns 1"){ REQUIRE(Fibonacci(1) ==
	1); }
TEST_CASE("Fibonacci(2) returns 2"){ REQUIRE(Fibonacci(2) ==
	2); }
TEST_CASE("Fibonacci(5) returns 8"){ REQUIRE(Fibonacci(5) ==
	8); }
\end{lstlisting}

If all these tests are compiled into the same executable called Fibonacci, then adding them with add\_test will only indicate that the executable failed, but not which of the scenarios seen in the previous code block.

The result of the test will look something like this:

\begin{tcblisting}{commandshell={}}
Test project /workspaces/CMake-Best-Practices/build
    Start 5: Fibonacci
1/1 Test #5: Fibonacci ........................***Failed
0.00 sec
0% tests passed, 1 tests failed out of 1
Total Test time (real) = 0.01 sec
The following tests FAILED:
         5 - Fibonacci (Failed)
\end{tcblisting}

That is hardly helpful to figure out which of the test cases failed. Luckily, with Catch2 and GoogleTest, there is a way to expose the internal tests to CTest so they are executed as regular tests. For GoogleTest, the module to do so is provided by CMake itself; Catch2 provides this functionality in its own CMake integration. Discovering the tests with Catch2 is done with catch\_discover\_tests, while for GoogleTest, gtest\_discover\_tests is used. The following example will expose tests written in the Catch2 framework to CTest:

\begin{lstlisting}[style=styleCMake]
find_package(Catch2)
include(Catch)
add_executable(Fibonacci)
catch_discover_tests(Fibonacci)
\end{lstlisting}

Note that, in order to have the function available, the Catch module has to be included. For GoogleTest it works very similarly:

\begin{lstlisting}[style=styleCMake]
include(GoogleTest)
add_executable(Fibonacci)
gtest_discover_tests(Fibonacci)
\end{lstlisting}

When using the discovery functions, each test case defined in a test executable will be treated as its own test by CTest. If tests are exposed like this, the result of a call to CTest might look as follows:

\begin{tcblisting}{commandshell={}}
    Start 5: Fibonacci(0) returns 1
1/4 Test #5: Fibonacci(0) returns 1 .........***Failed 0.00 sec
    Start 6: Fibonacci(1) returns 1
2/4 Test #6: Fibonacci(1) returns 1 ......... Passed 0.00 sec
    Start 7: Fibonacci(2) returns 2
3/4 Test #7: Fibonacci(2) returns 2 ......... Passed 0.00 sec
    Start 8: Fibonacci(5) returns 8
4/4 Test #8: Fibonacci(5) returns 8 ......... Passed 0.00 sec

75% tests passed, 1 tests failed out of 4
Total Test time (real) = 0.02 sec
The following tests FAILED:
         5 - Fibonacci(0) returns 1 (Failed)
\end{tcblisting}

Now, we see exactly which of the defined test cases failed. In this case, the Fibonacci(0) returns 1 test case did not behave as expected. This comes in especially handy when using an editor or IDE with integrated testing functionality. The discovery functions both work by running the specified executable with an option to only print out the test names to register them internally with CTest, so there is a slight overhead added to each build step. Discovering tests more granularly also has the advantage that the execution of them can be better parallelized by CMake, as described in the Running tests in parallel and managing test resources section of this chapter.

Both gtest\_discover\_tests and catch\_discover\_tests can take various options, such as adding a prefix or suffix to the test names or a list of properties to add to the generated tests. The full documentation for the functions can be found here:

\begin{itemize}
\item 
Catch2: \url{https://github.com/catchorg/Catch2/blob/devel/docs/cmake-integration.md}
	
\item 
GoogleTest: \url{https://cmake.org/cmake/help/v3.21/module/GoogleTest.html}
\end{itemize}

Catch2 and GoogleTest are just two of the many testing frameworks out there; there might be more test suites that bring this functionality with them that might be unknown to the authors. Now, let's move on from finding tests and have a closer look at how to control test behavior.

\subsubsubsection{7.2.2\hspace{0.2cm}Advanced ways to determine test success or failure}
 
By default, CTest determines whether a test failed or passed based on the return value of the command. 0 means all tests were successful, anything other than 0 is interpreted as a failure.

Sometimes, the return value is not enough to determine whether a test passes or fails. If you need to check program output for a certain string, the FAIL\_REGULAR\_EXPRESSION and PASS\_REGULAR\_EXPRESSION test properties can be used,  as shown in the following example:

\begin{lstlisting}[style=styleCMake]
set_tests_properties(some_test PROPERTIES
					FAIL_REGULAR_EXPRESSION "[W|w]arning|[E|e]rror"
					PASS_REGULAR_EXPRESSION "[S|s]uccess")
\end{lstlisting}

These properties would cause the some\_test test to fail if the output contains either "Warning" or "Error". If the "Success" string is found, the test is considered passed . If PASS\_REGULAR\_EXPRESSION is set, the test is considered passed only if thestring is present. In both cases, the return value will be ignored. If a certain return value of a test needs to be ignored, it can be passed with the SKIP\_RETURN\_CODE option.

Sometimes, a test is expected to fail. In those, setting WILL\_FAIL to true will cause the test result to be inverted:

\begin{lstlisting}[style=styleCMake]
add_test(NAME SomeFailingTerst COMMAND SomeFailingTest)
set_tests_properties(SomeFailingTest PROPERTIES WILL_FAIL True)
\end{lstlisting}

This is often better than disabling the test because the test will still be executed on each test run, and if the test unexpectedly starts to pass again, the developer is made aware of it. A special case of test failures is when tests fail to return or take too much time to complete. For this case, CTest provides the means of adding timeouts of tests and even retrying tests in the case of failure.

\subsubsubsection{7.2.3\hspace{0.2cm}Handling timeouts and repeating tests}

Sometimes, we're not just interested in the success or failure of a test, but also in how long it takes to complete. The TIMEOUT test property takes a number of seconds to determine a maximum runtime for a test. If the test exceeds the time, it is terminated and considered failed. The following command would limit the test execution of the test to 10 seconds:

\begin{lstlisting}[style=styleCMake]
set_tests_properties(timeout_test PROPERTIES TIMEOUT 10)
\end{lstlisting}

The TIMEOUT property often comes in handy for tests that run the risk of falling into infinite loops or hanging forever for whatever reason.

Alternatively, CTest accepts the -{}-timeout argument to set a global timeout that is applied to all tests that have no TIMEOUT property specified. For those tests that have TIMEOUT defined, the timeout defined in CmakeLists.txt takes precedence over the timeout passed over the command line.

To avoid long test execution, the CTest command line accepts the -{}-stop-time argument, which takes the real time of the day as a time limit for the complete set of tests. The following command would set a default timeout of 30 seconds for each test, and the tests would have to be completed before 23:59:

\begin{tcblisting}{commandshell={}}
ctest --timeout 30 --stop-time 23:59
\end{tcblisting}

ometimes, it can be expected for a test to experience occasional timeouts due to factors outside of our control. Very common cases are tests that need some form of network communication or a resource that has some kind of bandwidth limitation. Sometimes, the only way to get the test to run is to try it again. For this, the -{}-repeat aftertimeout:n command-line argument can be passed to CTest, where n is a number.

The -{}-repeat argument actually has three options:

\begin{itemize}
\item 
after-timeout: This retries the test a number of times if a timeout occurred. Generally, the -{}-timeout option should be passed to CTest whenever repeating after timeouts.

\item 
until-pass: This reruns a test until it passes or until the number of retries is reached. Setting this as a general rule in a CI environment is a bad idea, as tests should generally always pass.

\item 
until-fail: Tests are rerun a number of times or until they fail. This is often used if a test fails occasionally to find out how frequently this happens. The -{}-repeatuntil-fail argument works exactly like --repeat:until-fail:n. 
\end{itemize}

As mentioned, the reason for failing tests might be the unavailability of resources that the test depends on. One common case for external resources being unavailable is that they are flooded with requests from tests. The Running tests in parallel and managing test resources section describes a few options to avoid such complications. Another common cause for timeouts when accessing external resources is that the resource is not yet available when the tests run. In the next section, we will see how to write test fixtures that can be used to ensure that resources are started before a test run.

\subsubsubsection{7.2.4\hspace{0.2cm}Writing test fixtures}

Tests should, in general, be independent of each other. There are cases where tests might depend on a precondition that is not controlled by the test itself. For instance, a test might require a server to be running in order to test a client. These dependencies can be expressed in CMake by defining them as test fixtures using the FIXTURE\_SETUP, FIXTURE\_CLEANUP, and FIXTURE\_REQUIRED test properties. All three properties take a list of strings to identify the fixture. A test might indicate that it needs a particular fixture by defining the FIXTURE\_REQUIRED property. This will ensure that the test named as fixture completes successfully before it is executed. Similarly, a test might declare it in FIXTURE\_CLEANUP to indicate that it must be run after the completion of the test requiring the fixture. The fixtures defined in the cleanup part are always run, regardless of whether a test succeeded or failed. Consider the following example:

\begin{lstlisting}[style=styleCMake]
add_test(NAME start_server COMMAND echo_server --start)
set_tests_properties(start_server PROPERTIES FIXTURES_SETUP
	server)
add_test(NAME stop_server COMMAND echo_server --stop)
set_tests_properties(stop_server PROPERTIES FIXTURES_CLEANUP
	server)

add_test(NAME client_test COMMAND echo_client)
set_tests_properties(client_test PROPERTIES FIXTURES_REQUIRED
	server)
\end{lstlisting}

In this example, a program called echo\_server is used as a fixture so that another program called echo\_client can use it. The execution of echo\_server with the -{}-start and -{}-stop arguments is formulated as tests with the names start\_server and stop\_server. The start\_server test is marked as the setup of the fixture with the name server. The stop\_server test is set up likewise but is marked as the cleanup routine of the fixture. In the end, the actual test called client\_test is set up and it gets passed the server fixture as a required precondition.

If, now, the client\_test test is run using CTest, the fixtures are automatically invoked with it. The fixture tests show up as regular tests in the output for CTest, as shown in the following example output:

\begin{tcblisting}{commandshell={}}
ctest -R client
Test project CMake-Best-Practices:
    Start 9: start_server
1/3 Test #9: start_server .............. Passed 0.00 sec
    Start 11: client_test
2/3 Test #11: client_test................ Passed 0.00 sec
    Start 10: stop_server
3/3 Test #10: stop_server ............... Passed 0.00 sec
\end{tcblisting}

Note that CTest was invoked with a regex filter that only matches the client test, but that CTest starts the fixture anyway. To not overwhelm test fixtures when tests are executed in parallel, they can be defined as resources, as shown in the next section.

\subsubsubsection{7.2.5\hspace{0.2cm}Running tests in parallel and managing test resources}

If a project has many tests, executing them in parallel will speed up the tests. By default, CTest runs the tests serially; by passing the -j option to the call to CTest, the tests can be run in parallel. Alternatively, the number of parallel threads can be defined in the CTEST\_PARALLEL\_LEVEL environment variable. By default, CTest assumes that each test will run on a single CPU. If a test requires multiple processors to run successfully, setting the PROCESSORS property for the test can be set to define the number of processors required:

\begin{lstlisting}[style=styleCMake]
add_test(NAME concurrency_test COMMAND concurrency_tests)
set_tests_properties(concurrency_test PROPERTIES PROCESSORS 2)
\end{lstlisting}

This will tell CTest that the concurrency\_test test requires two CPUs to run. When running tests parallel with -j 8, concurrency\_test will occupy two of the eight available "slots" for parallel execution. If, in this case, the PROCESSORS property is set to 8, this would mean that no other test can run parallel to concurrency\_test. When setting a value for PROCESSORS that is higher than the available number of parallel slots or CPUs that are available on the system, the test will be run as soon as the full pool is available. Sometimes, there are tests that do not just require a specific amount of processors, but that need to run exclusively without any other test running. To achieve this, the RUN\_SERIAL property can be set to true for a test. This might have a serious impact on the overall test performance, so use this with caution. A more granular way to control this is by using the RESOURCE\_LOCK property, which contains a list of strings. The strings have no particular meaning, except that CTest prevents two tests from running in parallel if they list the same strings. In this way, partial serialization can be achieved without halting the whole test execution. It also is a nice way to specify whether tests need a particular unique resource, such as a certain file, a database, or similar. Consider the following example:

\begin{lstlisting}[style=styleCMake]
set_tests_properties(database_test_1 database_test_2 database_
	test_3 PROPERTIES RESOURCE_LOCK database)
set_tests_properties(some_other_test PROPERTIES RESOURCE_LOCK
	fileX)
set_tests_properties(yet_another_test PROPERTIES RESOURCE_LOCK
	"database;fileX ")
\end{lstlisting}

In this example, the database\_test\_1, database\_test\_2, and database\_test\_3 tests would be prevented from running in parallel. The some\_other\_test test will not be affected by the database tests, but yet\_another\_test will not run together with any of the database tests and some\_other\_test.

\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=Fixtures as Resources]
While not technically required, if RESOURCE\_LOCK is used together with FIXTURE\_SETUP, FIXTURE\_CLEANUP, and FIXTURE\_REQUIRED, it is good practice to use the same identifiers for the same resources.
\end{tcolorbox}

Managing parallelism of tests with RESOURCE\_LOCK is very handy when tests need exclusive access to some resource. In most cases, it is entirely sufficient to manage the parallelism. Since CMake 3.16, this can be controlled on an even more granular level with the RESOURCE\_GROUPS property. Resource groups allow not just the specification of what resources are used, but how much of a resource is used. Common scenarios are defining the amount of memory a particular greedy operation might need or avoiding overrunning the connection limit of a certain service. Resource groups often come into play when working on projects that use the GPU for general-purpose computing, to define how many slots of a GPU each test needs.

Resource groups are quite a step up in complexity compared to simple resource locks. To use them, CTest has to do the following things:

\begin{itemize}
\item 
Know which resources a test needs to run. This is defined by setting the test properties in the project.

\item 
Know which resources a system has available. This is done from outside the project when running the tests.

\item 
Pass the information on which resources to use for the test. This is done by using environment variables.
\end{itemize}

Like resource locks, resource groups are arbitrary strings used to identify resources. The definition of the actual resource tied to the label is left to the user. The resource groups are defined as name:value pairs, which are separated by commas if there are multiple groups. A test can define what resources to use with the RESOURCE\_GROUPS property, like this:

\begin{lstlisting}[style=styleCMake]
set_property(TEST SomeTest PROPERTY RESOURCE_GROUPS
	cpus:2,mem_mb:500
	servers:1,clients:1
	servers:1,clients:2
	4,servers:1,clients:1
)
\end{lstlisting}

In the preceding example, SomeTest states that it uses two CPUs and 500 MB of memory. It uses a total of six instances of a client-server pair, each pair having a number of servers and clients assigned. The first pair consists of one server instance and one client instance, the second pair requires one server but two client instances.

The last line, 4, servers:1,clients:1, is shorthand to tell CTest to use four instances of the same pair, consisting of one servers resource and one clients resource. This means this test will not run unless a total of six servers and seven clients are available, in addition to the required CPUs and memory.

The available system resources are specified in a JSON file that is passed to CTest, either by the ctest -{}-resource-spec-file command-line parameter or by setting the CTEST\_RESOURCE\_SPEC\_FILE variable when calling CMake. Setting the variable should be done by using cmake -D and not in CMakeLists.txt, as specifying the system resources should be done from outside the project.

A sample resource specification file for the preceding example could look like this:

\begin{lstlisting}[style=styleCMake]
{
	"version": {
		"major": 1,
		"minor": 0
	},
	"local": [
	{
		"mem_mb": [
			{
				"id": "memory_pool_0",
				"slots": 4096
			}
		],
		"cpus" :
		[
			{
				"id": "cpu_0",
				"slots": 8
			}
		],	
		"servers": [
			{
				"id": "0",
				"slots": 4
			},
			{
				"id": "1",
				"slots": 4
			}
		],
		"clients": [
		{
			"id": "0",
			"slots": 8
		},
		{
			"id": "1",
			"slots": 8
		}
		]
	}
	]
}
\end{lstlisting}

This file specifies a system with 4,096 MB of memory, eight CPUs, 2x4 server instances, and 2x8 client instances for a total of eight servers and 16 clients. If a resource request of a test cannot be satisfied with the available system resources, it fails to run with an error like this:

\begin{tcblisting}{commandshell={}}
ctest â€“j $(nproc) --resource-spec-file ../resources.json

Test project /workspaces/CMake-Best-Practices/chapter_7
  /resource_group_example/build
     Start 2: resource_test_2
                  Start 3: resource_test_3
Insufficient resources for test resource_test_3:

    Test requested resources of type 'mem_mb' in the following
        amounts:
        8096 slots
    but only the following units were available:
        'memory_pool_0': 4096 slots

Resource spec file:

  ../resources.json
\end{tcblisting}

The current example would be able to run with this specification, as it needs a total of six servers and seven clients. CTest has no way of ensuring that the specified resources are actually available or not; this is the task of the user or the CI system. For instance, a resource file might specify that there are eight CPUs available, while the hardware actually only contains four cores.

The information about the assigned resource groups is passed to the test over environment variables. The CTEST\_RESOURCE\_GROUP\_COUNT environment variable specifies the total number of resource groups assigned to a test. If it is not set, this means that CTest was invoked without an environment file. Tests should check this and act accordingly. If a test cannot run without the resources, it should either fail or indicate that it did not run by returning the respective return code or string defined in the SKIP\_RETURN\_CODE or SKIP\_REGULAR\_EXPRESSION property. The resource groups assigned to the test are passed with pairs of environment variables:

\begin{itemize}
\item 
CTEST\_RESOURCE\_GROUP\_<ID>, which will contain the type of the resource groups. In the example from earlier, this will be either "mem\_mb", "cpus", "clients", or "servers".

\item 
CTEST\_RESOURCE\_GROUP\_<ID>\_<TYPE>, which will contain a pair of id:slots for the types.
\end{itemize}

It is up to the implementation of the test on how the resource groups are used and internally distributed.

Writing and running tests is obviously one of the major boosters of code quality. However, another interesting metric is often how much of your code is actually covered by the tests. Surveying and reporting code coverage can give interesting hints, not just about how widely software is tested, but also about where the gaps lie.


















